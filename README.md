# ğŸ§  Aqumen.ai Adversarial Assessment System

A sophisticated multi-model adversarial pipeline for testing domain expertise in AI/ML development, featuring real-time error detection and interactive assessments.

## ğŸ—ï¸ Project Structure

```
ğŸ“ adversarial demo/
â”œâ”€â”€ ğŸ“ frontend/           # React components and assets
â”‚   â”œâ”€â”€ ğŸ“ components/     # Modular React components  
â”‚   â”œâ”€â”€ ğŸ“ src/           # Main React application
â”‚   â”œâ”€â”€ constants.js      # Sample data and configurations
â”‚   â”œâ”€â”€ hooks.js          # Custom React hooks
â”‚   â”œâ”€â”€ utils.js          # Utility functions
â”‚   â””â”€â”€ package.json      # Frontend dependencies
â”œâ”€â”€ ğŸ“ backend/           # Python backend and API
â”‚   â”œâ”€â”€ corrected_7step_pipeline.py  # Core 7-step pipeline orchestrator
â”‚   â”œâ”€â”€ api_server.py     # FastAPI service with streaming support
â”‚   â”œâ”€â”€ aqumen_pipeline/  # Shared prompts, tools, validators, datatypes
â”‚   â”œâ”€â”€ analytics/        # Reward telemetry calculations
â”‚   â”œâ”€â”€ clients/          # Bedrock runtime wrapper
â”‚   â”œâ”€â”€ config/           # Prompt/tool configuration loaders
â”‚   â”œâ”€â”€ persistence/      # SQLite repository for pipeline runs
â”‚   â”œâ”€â”€ services/         # Invocation helpers
â”‚   â”œâ”€â”€ prompts.json      # Canonical prompt templates
â”‚   â”œâ”€â”€ tools.json        # JSON schemas exposed to LLM tools
â”‚   â”œâ”€â”€ streamlit_app.py  # Streamlit demo application
â”‚   â””â”€â”€ requirements.txt  # Python dependencies  
â”œâ”€â”€ ğŸ“ examples/          # Gold standard examples and evaluations
â”‚   â”œâ”€â”€ gold_standard_examples.md      # Complete pipeline examples
â”‚   â”œâ”€â”€ hardcoded_demo_data.js        # Demo data for frontend
â”‚   â”œâ”€â”€ adversarial_example_rubric.md # Evaluation framework
â”‚   â””â”€â”€ improved_example_evaluations.md # Score analysis
â”œâ”€â”€ ğŸ“ docs/             # Documentation and setup guides
â”‚   â”œâ”€â”€ AWS_BEDROCK_SETUP.md          # AWS integration guide
â”‚   â”œâ”€â”€ prompt_improvements.md        # Prompt engineering details
â”‚   â””â”€â”€ *.md                          # Various documentation files
â””â”€â”€ README.md            # This file
```

## âœ¨ Features

### ğŸ¯ 7-Step Adversarial Pipeline
1. **Difficulty Categories** - Generate skill-based progression with time indicators
2. **Error Catalog** - Create domain-specific conceptual error database  
3. **Adversarial Question** - Generate questions targeting specific knowledge gaps
4. **Model Testing** - Test strong vs weak models on the question
5. **Response Judgment** - Evaluate model differentiation with confidence scoring
6. **Student Assessment** - Transform into educational interactive exercises
7. **Pipeline Analytics** - Track quality metrics and success rates

### ğŸ† Gold Standard Examples (Scores 5.3-5.6/5.8)
Based on **DeepLearning.AI 2024-2025 course materials**:

1. **LLM Post-Training Pipeline** (5.36) - Tests RLHF sequence understanding
2. **Knowledge Graph API Discovery** (5.56) - Tests semantic reasoning vs logical compatibility  
3. **Multi-Agent Coordination** (5.56) - Tests distributed systems expertise

### ğŸš€ Demo Modes
- **React Frontend** - Interactive error detection with clickable spans
- **Streamlit Backend** - Web-based pipeline demonstration
- **API Integration** - Real AWS Bedrock Claude model calls
- **Fallback Mode** - Hardcoded examples when API unavailable

## ğŸ› ï¸ Setup Instructions

### Frontend (React)
```bash
cd frontend/
npm install
npm run dev
# Opens at http://localhost:3000
# Mobile access: http://[your-ip]:3000 (same WiFi)
```

### Backend (Streamlit)  
```bash
cd backend/
pip install -r requirements.txt
streamlit run streamlit_app.py
# Opens at http://localhost:8501
# Mobile access: http://[your-ip]:8501 (same WiFi)
```

### AWS Bedrock Integration (Optional)
See `docs/AWS_BEDROCK_SETUP.md` for:
- AWS credentials configuration
- Model permissions setup
- Streamlit secrets configuration

## ğŸš€ Deployment Options

### Local Development
- **Desktop**: `http://localhost:3000` (React) or `http://localhost:8501` (Streamlit)
- **Mobile**: `http://[your-computer-ip]:3000` or `http://[your-computer-ip]:8501`
- **Duration**: Runs until terminal closed or computer restarts
- **Setup**: Ensure both devices on same WiFi network

### Production Deployment

#### GitHub Pages (Static Hosting)
```bash
# After pushing to GitHub
# Enable GitHub Pages in repo settings  
# Demo live at: https://yourusername.github.io/repo-name/
```

#### Vercel (React-Optimized)
```bash
npm i -g vercel
cd frontend/
vercel
# Demo live at: https://your-project.vercel.app/
```

#### Streamlit Cloud (Python Apps)
```bash
# 1. Push to GitHub (public repo)
# 2. Go to share.streamlit.io
# 3. Connect GitHub repo
# 4. Deploy backend/streamlit_app.py
# 5. Live at: https://share.streamlit.io/username/repo/streamlit_app.py
```

### Platform Comparison

| Platform | Best For | Cost | Features |
|----------|----------|------|----------|
| **GitHub Pages** | Static demos | Free | Simple hosting, custom domains |
| **Vercel** | React apps | Free tier | Fast CDN, automatic deployments |
| **Netlify** | JAMstack apps | Free tier | Form handling, serverless functions |
| **Streamlit Cloud** | Python demos | Free | ML-focused, handles dependencies |

## ğŸ“Š Evaluation Framework

Examples are scored using a comprehensive rubric:

### Output Quality (60% weight)
- **Conceptual Depth** (25%) - Tests fundamental domain understanding
- **Subtlety & Adversarial Quality** (20%) - Error difficulty to spot
- **Production Impact** (20%) - Real-world consequences  
- **Expert Differentiation** (15%) - Strong vs weak model separation
- **Educational Value** (10%) - Learning benefit
- **Realistic Context** (10%) - Production relevance

### Pipeline Demonstration (40% weight)  
- **Error Catalog Quality** (20%) - Domain-specific error sophistication
- **Model Differentiation** (20%) - Clear expert knowledge gaps
- **Difficulty Categories** (15%) - Skill progression quality
- **Judgment Sophistication** (15%) - Evaluation system quality
- **Student Assessment** (15%) - Educational transformation
- **Pipeline Flow** (15%) - Coherent step-by-step narrative

**Score Ranges:**
- ğŸ¥‡ **4.5-5.0+**: Gold Standard (demo-ready)
- ğŸ¥ˆ **4.0-4.4**: Excellent  
- ğŸ¥‰ **3.5-3.9**: Good
- âšª **3.0-3.4**: Fair
- ğŸ”´ **<3.0**: Poor

## ğŸ® Usage Examples

### Interactive Error Detection
```javascript
// React component usage
import { samplePipelineData } from './constants.js';

const question = samplePipelineData['LLM Post-Training with DPO'].finalQuestion;
// Renders code with clickable error spans
// User clicks spans to identify conceptual errors
// System evaluates precision/recall/F1 scores
```

### Pipeline Simulation
```python
# Streamlit demo
topic = "Knowledge Graphs for AI Agent API Discovery"
result = run_adversarial_pipeline(topic)
# Executes full 7-step pipeline with progress tracking
# Shows model differentiation and quality metrics
```

### API Integration
```python
# Real Bedrock API calls
from corrected_7step_pipeline import CorrectedSevenStepPipeline

pipeline = CorrectedSevenStepPipeline()
result = pipeline.run_full_pipeline("Multi AI Agent Systems with crewAI")
# Returns SevenStepResult with rich metadata + saved telemetry
```

## ğŸ”¬ Research Applications

### Academic Use Cases
- **AI Education** - Interactive error detection exercises
- **Model Evaluation** - Systematic capability assessment
- **Curriculum Design** - Skill progression mapping
- **Assessment Creation** - Automated question generation

### Industry Applications  
- **Technical Interviews** - Domain expertise verification
- **Training Programs** - Skill gap identification
- **Model Testing** - Production readiness validation
- **Quality Assurance** - Code review automation

## ğŸ¤ Contributing

The system is designed for extensibility:

1. **Add New Domains** - Extend `samplePipelineData` in `constants.js`
2. **Improve Prompts** - Update `backend/prompts.json` (or overrides in `prompts_changes.json`)
3. **Create Examples** - Use evaluation rubric in `examples/`
4. **Enhance UI** - Develop new React components in `frontend/components/`

## ğŸ“ˆ Performance Metrics

Current gold standard examples achieve:
- **Perfect Conceptual Depth** (5.0/5.0) - Tests fundamental domain theory
- **Maximum Subtlety** (5.0/5.0) - Errors look completely reasonable  
- **Critical Impact** (5.0/5.0) - Production failure consequences
- **Clear Differentiation** (5.0/5.0) - Expert vs intermediate knowledge gaps

## ğŸ”® Future Enhancements

- **Multi-domain Pipeline** - Cross-cutting AI expertise testing
- **Adaptive Difficulty** - Dynamic question complexity adjustment  
- **Real-time Analytics** - Live assessment performance tracking
- **Collaborative Mode** - Team-based error detection exercises
- **API Marketplace** - Third-party domain extensions

## ğŸ“„ License

[Add your license information here]

---

**Built with:** React, Streamlit, AWS Bedrock, Claude AI, Python, JavaScript

**Powered by:** DeepLearning.AI 2024-2025 course materials and cutting-edge adversarial assessment research
