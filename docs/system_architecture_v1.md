# ğŸ—ï¸ System Architecture & Data Flow Diagrams v1.0

## 1. ğŸ¯ **High-Level System Architecture**

```mermaid
graph TB
    User[ğŸ‘¤ User Input<br/>Topic Selection] --> Router{ğŸ”€ Topic Router}
    
    Router --> Demo[ğŸ“± Demo Mode<br/>Hardcoded Examples]
    Router --> Live[ğŸš€ Live Generation<br/>7-Step Pipeline]
    
    Demo --> Frontend[ğŸ–¥ï¸ Frontend Display]
    Live --> Pipeline[âš™ï¸ 7-Step Pipeline Engine]
    
    Pipeline --> DB[(ğŸ—„ï¸ Results Database)]
    Pipeline --> Frontend
    
    DB --> Analytics[ğŸ“Š Analytics Dashboard]
    Frontend --> Student[ğŸ‘¨â€ğŸ“ Student Assessment Interface]
```

## 2. ğŸ”„ **7-Step Pipeline Data Flow**

```mermaid
graph TD
    A[ğŸ“ Topic Input] --> B[ğŸ·ï¸ Step 1: Difficulty Categories]
    B --> C[âš ï¸ Step 2: Error Catalog]
    C --> D[ğŸ¯ Step 3: Strategic Question]
    D --> E[ğŸ’ª Step 4: Strong Model Implementation]
    D --> F[ğŸ˜” Step 5: Weak Model Implementation]
    E --> G[âš–ï¸ Step 6: Judge Differentiation]
    F --> G
    G --> H{âœ… Differentiation?}
    H -->|YES| I[ğŸ“ Step 7: Student Assessment]
    H -->|NO| J[ğŸ”„ Retry with Feedback]
    J --> D
    I --> K[ğŸ’¾ Save to Database]
    K --> L[ğŸ“Š Rubric Scoring]
    L --> M[ğŸ¯ Final Output]

    subgraph "ğŸ¤– Model Assignments"
        N[Sonnet 3.5 v2<br/>Steps 1,2,3,6,7]
        O[Haiku 3.5<br/>Step 4 - Mid-tier]
        P[Haiku 3<br/>Step 5 - Weak-tier]
    end
```

## 3. ğŸ§  **Agent Architecture Pattern**

```mermaid
graph LR
    subgraph "ğŸ¯ Strategic Agent (Sonnet 3.5 v2)"
        A1[Question Designer]
        A2[Error Cataloger]
        A3[Judge Evaluator]
        A4[Assessment Creator]
    end
    
    subgraph "ğŸ› ï¸ Implementation Agents"
        B1[Strong Implementer<br/>Haiku 3.5]
        B2[Weak Implementer<br/>Haiku 3]
    end
    
    subgraph "ğŸ“Š System Agents"
        C1[Rubric Scorer]
        C2[Database Manager]
        C3[Analytics Engine]
    end
    
    A1 --> B1
    A1 --> B2
    B1 --> A3
    B2 --> A3
    A3 --> A4
    A4 --> C1
    C1 --> C2
    C2 --> C3
```

## 4. ğŸ—ƒï¸ **Data Storage Architecture**

```mermaid
erDiagram
    PIPELINE_RESULTS {
        int id PK
        string timestamp
        string topic
        string subtopic
        string difficulty
        boolean differentiation_achieved
        int stopped_at_step
        int total_attempts
        json weak_model_failures
        boolean step_6_success
        string pipeline_version
    }
    
    RUBRIC_SCORES {
        int id PK
        int pipeline_result_id FK
        string timestamp
        string topic
        float output_quality_total
        float pipeline_demonstration_total
        float base_final_score
        float bonus_penalty_score
        float final_score
        string score_classification
        float conceptual_depth
        float strategic_design
        float production_impact
        float expert_differentiation
        float educational_value
        float realistic_context
    }
    
    HARDCODED_EXAMPLES {
        int id PK
        string topic_category
        string audience_type
        json full_pipeline_result
        float rubric_score
        string demo_priority
        timestamp created_at
    }
    
    USER_SESSIONS {
        int id PK
        string session_id
        string user_type
        json topics_generated
        timestamp started_at
        int total_assessments
    }
    
    PIPELINE_RESULTS ||--o{ RUBRIC_SCORES : "has scores"
    PIPELINE_RESULTS ||--o{ USER_SESSIONS : "belongs to session"
```

## 5. ğŸŒ **System Integration Flow**

```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ User
    participant Frontend as ğŸ–¥ï¸ Frontend
    participant API as ğŸ”Œ API Gateway
    participant Pipeline as âš™ï¸ Pipeline Engine
    participant Models as ğŸ¤– Claude Models (AWS Bedrock)
    participant DB as ğŸ—„ï¸ Database
    participant Scorer as ğŸ“Š Rubric Scorer
    
    User->>Frontend: Select Topic
    Frontend->>API: POST /generate-assessment
    API->>Pipeline: Initialize 7-step process
    
    loop Steps 1-7
        Pipeline->>Models: API call with structured schema
        Models-->>Pipeline: Structured JSON response
        Pipeline->>DB: Log step result
    end
    
    Pipeline->>Scorer: Score completed pipeline
    Scorer->>DB: Save rubric scores
    Pipeline-->>API: Return final result
    API-->>Frontend: Assessment + metadata
    Frontend-->>User: Display interactive assessment
```

## 6. ğŸª **Demo Mode Architecture**

```mermaid
graph TD
    A[ğŸ¯ Topic Selection] --> B{ğŸ” Topic Matcher}
    B --> C[ğŸ“š Hardcoded Examples DB]
    C --> D[ğŸ† Gold Standard Examples]
    D --> E[ğŸ“± Instant Display]
    
    B --> F[ğŸš€ Live Generation Fallback]
    F --> G[âš™ï¸ 7-Step Pipeline]
    G --> H[â±ï¸ 5-min timeout]
    H --> I[ğŸ’¾ Cache Result]
    
    subgraph "ğŸ­ Demo Categories"
        J[ğŸ”§ Technical<br/>DPO, Multi-Agent, etc.]
        K[ğŸ’¼ Business<br/>Executive Assistant, etc.]
        L[ğŸ“Š Data Science<br/>Analytics, ML, etc.]
    end
```

## 7. ğŸ¨ **Frontend Component Architecture**

```mermaid
graph TB
    subgraph "ğŸ“± Main App"
        A[ğŸ  Home/Topic Selection]
        B[âš™ï¸ Generation Progress]
        C[ğŸ¯ Assessment Display]
        D[ğŸ“Š Results Dashboard]
    end
    
    subgraph "ğŸ§© Reusable Components"
        E[ğŸª Demo Mode Toggle]
        F[ğŸ“‹ Topic Categories]
        G[ğŸ”„ Progress Indicator]
        H[ğŸ’¬ Error Span Highlighter]
        I[ğŸ“ˆ Score Visualizer]
    end
    
    subgraph "ğŸ“ Student Interface"
        J[ğŸ“ Interactive Code View]
        K[ğŸ–±ï¸ Clickable Error Spans]
        L[ğŸ’¡ Learning Objectives]
        M[ğŸ“š Concept Explanations]
    end
    
    A --> E
    A --> F
    B --> G
    C --> H
    C --> J
    D --> I
    J --> K
    K --> L
    L --> M
```

## 8. ğŸš€ **Deployment Architecture**

```mermaid
graph TB
    subgraph "â˜ï¸ Cloud Infrastructure"
        A[ğŸŒ Load Balancer]
        B[ğŸ–¥ï¸ Frontend Servers<br/>React/Next.js]
        C[âš™ï¸ API Servers<br/>Python/FastAPI]
        D[ğŸ—„ï¸ Database<br/>PostgreSQL/SQLite]
    end
    
    subgraph "ğŸ¤– External Services"
        E[ğŸ”— AWS Bedrock<br/>Claude Models]
        F[ğŸ“Š Analytics Service]
        G[ğŸ” Auth Provider]
    end
    
    A --> B
    A --> C
    C --> D
    C --> E
    B --> F
    B --> G
    
    subgraph "ğŸ”§ Development"
        H[ğŸ³ Docker Containers]
        I[ğŸš€ CI/CD Pipeline]
        J[ğŸ“‹ Environment Configs]
    end
```

## 9. ğŸ“Š **Analytics & Monitoring Flow**

```mermaid
graph LR
    A[ğŸ“ˆ Pipeline Metrics] --> B[ğŸ“Š Analytics Engine]
    C[ğŸ‘¤ User Interactions] --> B
    D[ğŸ¤– Model Performance] --> B
    
    B --> E[ğŸ“‹ Success Rate Dashboard]
    B --> F[ğŸ¯ Topic Performance Analysis]
    B --> G[âš ï¸ Failure Pattern Detection]
    B --> H[ğŸ”” Alert System]
    
    E --> I[ğŸ’¼ Business Intelligence]
    F --> I
    G --> J[ğŸ› ï¸ System Optimization]
    H --> K[ğŸ“ Support Notifications]
```

## 10. ğŸ”„ **Feedback & Improvement Loop**

```mermaid
graph TD
    A[ğŸ“Š System Metrics] --> B[ğŸ§  Analysis Engine]
    C[ğŸ‘¤ User Feedback] --> B
    D[ğŸ“ˆ Success Rates] --> B
    
    B --> E{ğŸ“‰ Performance Issues?}
    E -->|YES| F[ğŸ› ï¸ Prompt Engineering]
    E -->|YES| G[ğŸ”§ Model Retraining]
    E -->|NO| H[âœ… System Healthy]
    
    F --> I[ğŸ§ª A/B Testing]
    G --> I
    I --> J[ğŸ“Š Validation]
    J --> K[ğŸš€ Deployment]
    K --> A
```

## 11. ğŸ”§ **Structured Output Flow**

```mermaid
graph TD
    A[ğŸ“ Model Prompt] --> B[ğŸ› ï¸ Tool Schema Definition]
    B --> C[ğŸ¤– Claude API Call]
    C --> D[ğŸ“Š Structured JSON Response]
    D --> E{âœ… Valid Structure?}
    E -->|YES| F[âœ… Process Data]
    E -->|NO| G[ğŸ”„ Retry with Fallback]
    G --> C
    F --> H[ğŸ’¾ Store in Database]
    
    subgraph "ğŸ¯ Tool Schema Types"
        I[ğŸ“‹ Difficulty Categories]
        J[âš ï¸ Error Catalog]
        K[ğŸ¯ Strategic Question]
        L[ğŸ“ Student Assessment]
    end
    
    B --> I
    B --> J
    B --> K
    B --> L
```

## 12. ğŸ¯ **Topic Routing Logic**

```mermaid
graph TD
    A[ğŸ‘¤ User Topic Input] --> B[ğŸ” Topic Analyzer]
    B --> C{ğŸ“š Existing Example?}
    C -->|YES| D[ğŸ† Gold Standard Match]
    C -->|NO| E[ğŸ¯ Domain Classifier]
    
    D --> F[ğŸ“± Instant Demo Display]
    
    E --> G{ğŸ”§ Technical Domain?}
    E --> H{ğŸ’¼ Business Domain?}
    E --> I{ğŸ“Š Data Science Domain?}
    
    G -->|YES| J[âš™ï¸ Technical Pipeline Config]
    H -->|YES| K[ğŸ’¼ Business Pipeline Config]
    I -->|YES| L[ğŸ“Š Analytics Pipeline Config]
    
    G -->|NO| M[ğŸŒ General Pipeline Config]
    H -->|NO| M
    I -->|NO| M
    
    J --> N[ğŸš€ Live Generation]
    K --> N
    L --> N
    M --> N
    
    N --> O[ğŸ“Š Results + Caching]
```

## 13. ğŸª **Demo vs Live Generation**

```mermaid
graph LR
    subgraph "ğŸª Demo Mode (Fast)"
        A[âš¡ Instant Response]
        B[ğŸ† Pre-validated Quality]
        C[ğŸ“š Curated Examples]
    end
    
    subgraph "ğŸš€ Live Generation (Thorough)"
        D[â±ï¸ 3-5 Minutes]
        E[ğŸ¯ Custom Topic]
        F[ğŸ”„ Real-time Pipeline]
    end
    
    subgraph "ğŸ”€ Hybrid Strategy"
        G[ğŸ¯ Topic Match Algorithm]
        H[âš¡ Progressive Enhancement]
        I[ğŸ’¾ Smart Caching]
    end
    
    A --> G
    D --> G
    B --> H
    E --> H
    C --> I
    F --> I
```

## 14. ğŸ“± **Mobile-First Architecture**

```mermaid
graph TB
    subgraph "ğŸ“± Mobile Interface"
        A[ğŸ“‹ Simplified Topic Selection]
        B[ğŸ‘† Touch-Optimized Error Spans]
        C[ğŸ“Š Condensed Progress View]
    end
    
    subgraph "ğŸ’» Desktop Interface"
        D[ğŸ–±ï¸ Advanced Topic Filtering]
        E[âŒ¨ï¸ Keyboard Shortcuts]
        F[ğŸ“ˆ Detailed Analytics]
    end
    
    subgraph "ğŸ”„ Responsive Core"
        G[ğŸ“ Adaptive Layouts]
        H[ğŸ¯ Progressive Disclosure]
        I[âš¡ Performance Optimization]
    end
    
    A --> G
    B --> H
    C --> I
    D --> G
    E --> H
    F --> I
```

## 15. ğŸ›¡ï¸ **Error Handling & Resilience**

```mermaid
graph TD
    A[ğŸ¯ Pipeline Step] --> B{âš ï¸ Error Occurred?}
    B -->|NO| C[âœ… Continue Pipeline]
    B -->|YES| D[ğŸ” Error Classification]
    
    D --> E{ğŸ¤– API Error?}
    D --> F{ğŸ“Š Parsing Error?}
    D --> G{â±ï¸ Timeout?}
    
    E -->|YES| H[ğŸ”„ Exponential Backoff]
    F -->|YES| I[ğŸ› ï¸ Structured Output Retry]
    G -->|YES| J[â­ï¸ Skip to Demo Mode]
    
    H --> K[ğŸ”¢ Max Retries?]
    I --> K
    K -->|NO| A
    K -->|YES| L[ğŸª Fallback to Demo]
    
    J --> L
    L --> M[ğŸ“ Log Failure]
    M --> N[ğŸ”” Alert System]
```

---

## ğŸ¯ **Key Architectural Decisions**

### **1. Multi-Agent Orchestration**
- **Strategic Agent** (Sonnet 3.5 v2) handles complex reasoning tasks (Steps 1,2,3,6,7)
- **Implementation Agents** (Haiku 3.5, Haiku 3) provide differentiated responses (Steps 4,5)
- **System Agents** handle scoring, storage, and analytics

### **2. Structured Data Flow**
- Tool schemas enforce consistent JSON structure across all pipeline steps
- Pipeline results stored with full audit trail for analysis
- Automated rubric scoring with detailed breakdowns

### **3. Hybrid Demo Architecture**
- Hardcoded gold standard examples for reliable, fast demos
- Live generation with comprehensive fallback mechanisms
- Topic-specific caching for performance optimization

### **4. Scalable Storage Design**
- Relational database for structured queries and analytics
- JSON storage for flexible pipeline data and configurations
- Analytics-optimized data models for performance insights

### **5. Fail-Safe Mechanisms**
- Multiple retry attempts with intelligent backoff strategies
- Timeout handling with graceful degradation to cached examples
- Comprehensive error logging and alerting systems

### **6. Performance Optimization**
- Structured outputs reduce parsing failures
- Caching strategy balances freshness with speed
- Progressive enhancement for different user contexts

### **7. Domain Flexibility**
- Pipeline configuration adapts to technical, business, or general domains
- Rubric system works across different knowledge areas
- Topic routing optimizes experience based on content type

---

## ğŸš€ **Implementation Priorities**

### **Phase 1: Core Stability**
1. âœ… Structured outputs for all JSON parsing steps
2. âœ… Database integration with comprehensive logging
3. âœ… Gold standard examples for demo mode
4. ğŸ”„ Error handling and retry mechanisms

### **Phase 2: User Experience**
1. ğŸ¯ Frontend interface with interactive error spans
2. ğŸ“Š Real-time progress indicators
3. ğŸª Demo vs live mode switching
4. ğŸ“± Mobile-responsive design

### **Phase 3: Intelligence & Analytics**
1. ğŸ“ˆ Analytics dashboard for success metrics
2. ğŸ§  Topic performance analysis
3. ğŸ”„ Automated prompt optimization
4. ğŸ’¡ Intelligent topic suggestions

### **Phase 4: Advanced Features**
1. ğŸ“ Multi-user session management
2. ğŸ›¡ï¸ Anti-cheating integration
3. ğŸŒ API for third-party integrations
4. ğŸ¤– Advanced AI orchestration

This architecture provides a solid foundation for both the current system and future enhancements aligned with the big vision.